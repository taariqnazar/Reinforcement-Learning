{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f82cc0-0753-4109-b448-7ddc636c99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import sys\n",
    "sys.path.append('../lab0')\n",
    "import maze as mz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "491411db-cb16-41ba-a7a5-691e38672520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implemented methods\n",
    "methods = ['DynProg', 'ValIter'];\n",
    "\n",
    "# Some colours\n",
    "LIGHT_RED    = '#FFC4CC';\n",
    "LIGHT_GREEN  = '#95FD99';\n",
    "BLACK        = '#000000';\n",
    "WHITE        = '#FFFFFF';\n",
    "LIGHT_PURPLE = '#E8D0FF';\n",
    "LIGHT_ORANGE = '#FAE0C3';\n",
    "\n",
    "class Maze1:\n",
    "     # Actions\n",
    "    STAY       = 0\n",
    "    MOVE_LEFT  = 1\n",
    "    MOVE_RIGHT = 2\n",
    "    MOVE_UP    = 3\n",
    "    MOVE_DOWN  = 4\n",
    "\n",
    "    # Give names to actions\n",
    "    actions_names = {\n",
    "        STAY: \"stay\",\n",
    "        MOVE_LEFT: \"move left\",\n",
    "        MOVE_RIGHT: \"move right\",\n",
    "        MOVE_UP: \"move up\",\n",
    "        MOVE_DOWN: \"move down\"\n",
    "    }\n",
    "    \n",
    "    def __init__(self, maze, weights=None, STEP_REWARD = 0, GOAL_REWARD = 0, random_rewards=False, min_move=1):\n",
    "        \"\"\" Constructor of the environment Maze.\n",
    "        \"\"\"\n",
    "        self.maze                     = maze;\n",
    "        self.win_state = self.__win_state()\n",
    "        self.actions                  = self.__actions();\n",
    "        self.states, self.map         = self.__states();\n",
    "        self.n_actions                = len(self.actions);\n",
    "        self.n_states                 = len(self.states);\n",
    "        self.min_move = min_move\n",
    "        self.transition_probabilities = self.__transitions();\n",
    "        \n",
    "        self.STEP_REWARD = STEP_REWARD\n",
    "        self.GOAL_REWARD = GOAL_REWARD\n",
    "        self.rewards                  = self.__rewards(weights=weights,\n",
    "                                                random_rewards=random_rewards);\n",
    "        \n",
    "        \n",
    "    def __win_state(self):\n",
    "        return tuple([i[0] for i in np.where(self.maze == 2)])        \n",
    "        \n",
    "    def __actions(self):\n",
    "        actions = dict();\n",
    "        actions[self.STAY]       = (0, 0);\n",
    "        actions[self.MOVE_LEFT]  = (0,-1);\n",
    "        actions[self.MOVE_RIGHT] = (0, 1);\n",
    "        actions[self.MOVE_UP]    = (-1,0);\n",
    "        actions[self.MOVE_DOWN]  = (1,0);\n",
    "        return actions;\n",
    "    \n",
    "    def __states(self):\n",
    "        states = dict()\n",
    "        map = dict()\n",
    "        s = 0\n",
    "        for i in range(self.maze.shape[0]):\n",
    "            for j in range(self.maze.shape[1]):\n",
    "                if maze[i,j] !=1:\n",
    "                    for k in range(self.maze.shape[0]):\n",
    "                        for m in range(self.maze.shape[1]):\n",
    "                            if maze[k,m] !=1:\n",
    "                                if ((i,j) != (k,m) and (i,j) != self.win_state):\n",
    "                                    states[s] = (i,j,k,m)\n",
    "                                    map[(i,j,k,m)] = s\n",
    "                                    s += 1\n",
    "\n",
    "        states[s] = (1,1,1,1) # Win state \n",
    "        map[(1,1,1,1)] = s\n",
    "        s+=1\n",
    "\n",
    "        states[s] = (-1,-1,-1,-1) #Lost state\n",
    "        map[(-1,-1,-1,-1)] = s\n",
    "        return states, map\n",
    "\n",
    "    def hitting_maze_walls(self,row, col):\n",
    "        return (row == -1) or (row == self.maze.shape[0]) or \\\n",
    "                  (col == -1) or (col == self.maze.shape[1]) or \\\n",
    "                  (self.maze[row,col] == 1);\n",
    "\n",
    "    def __moves(self,state,action):\n",
    "        #If win or loose stay there\n",
    "        if state == self.map[(-1,-1,-1,-1)] or state == self.map[(1,1,1,1)]:\n",
    "            return [state]\n",
    "        s = np.array(self.states[state])\n",
    "        \n",
    "        #Check viable moves\n",
    "        p_next = tuple(s[:2] + self.actions[action])\n",
    "        if self.hitting_maze_walls(p_next[0], p_next[1]):\n",
    "                  p_next = tuple(s[:2])\n",
    "                \n",
    "        mt_temp = [tuple(s[2:] + self.actions[i]) for i in range(self.min_move,5)]\n",
    "        mt_next= []\n",
    "        for mt in mt_temp:\n",
    "            if not self.hitting_maze_walls(mt[0], mt[1]):\n",
    "                  mt_next.append(mt)\n",
    "        \n",
    "        s_next = [p_next + mt for mt in mt_next]\n",
    "\n",
    "        #Check if game is finished\n",
    "        final = []\n",
    "        for i in s_next:\n",
    "            if i[:2] == i[2:]:\n",
    "                final.append((-1,-1,-1,-1))\n",
    "            elif i[:2] == self.win_state:\n",
    "                final.append((1,1,1,1))\n",
    "            else:\n",
    "                final.append(i)\n",
    "\n",
    "        if len(final) < 1:\n",
    "            return [state]\n",
    "        else:\n",
    "            return [self.map[i] for i in final]\n",
    "        \n",
    "    def __transitions(self):\n",
    "        dimensions = (self.n_states,self.n_states,self.n_actions)\n",
    "        transition_probabilities = np.zeros(dimensions)\n",
    "\n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                next_ss = self.__moves(s,a)\n",
    "                for next_s in next_ss:\n",
    "                    transition_probabilities[next_s,s,a] += 1/len(next_ss)\n",
    "\n",
    "        return transition_probabilities\n",
    "    \n",
    "    def __rewards(self, weights=None, random_rewards=None):\n",
    "        rewards = np.zeros((self.n_states, self.n_actions))\n",
    "        \n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                next_s = self.__moves(s,a)\n",
    "                for ns in next_s:\n",
    "                    if ns == self.n_states-2:\n",
    "                        rewards[s,a] = self.GOAL_REWARD\n",
    "                    #elif ns == self.n_states-1:\n",
    "                    #    rewards[s,a] = -self.GOAL_REWARD\n",
    "                    else:\n",
    "                        rewards[s,a] = self.STEP_REWARD\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    def _move(self,state, action):\n",
    "        next_s = self.__moves(state, action)\n",
    "        return np.random.choice(next_s)\n",
    "        \n",
    "    def simulate(self, start, policy, method='DynProg'):\n",
    "        if method not in methods:\n",
    "            error = 'ERROR: the argument method must be in {}'.format(methods);\n",
    "            raise NameError(error);\n",
    "\n",
    "        path = list();\n",
    "        if method == 'DynProg':\n",
    "            # Deduce the horizon from the policy shape\n",
    "            horizon = policy.shape[1];\n",
    "            # Initialize current state and time\n",
    "            t = 0;\n",
    "            s = self.map[start];\n",
    "            # Add the starting position in the maze to the path\n",
    "            path.append(start);\n",
    "            while t < horizon-1:\n",
    "                # Move to next state given the policy and the current state\n",
    "                next_s = self._move(s,policy[s,t]);\n",
    "                # Add the position in the maze corresponding to the next state\n",
    "                # to the path\n",
    "                path.append(self.states[next_s])\n",
    "                # Update time and state for next iteration\n",
    "                t +=1;\n",
    "                s = next_s;\n",
    "        \n",
    "        if method == 'ValIter':\n",
    "            # Initialize current state, next state and time\n",
    "            t = 1;\n",
    "            s = self.map[start];\n",
    "            # Add the starting position in the maze to the path\n",
    "            path.append(start);\n",
    "            # Move to next state given the policy and the current state\n",
    "            next_s = self._move(s,policy[s]);\n",
    "            # Add the position in the maze corresponding to the next state\n",
    "            # to the path\n",
    "            path.append(self.states[next_s]);\n",
    "            \n",
    "            # Loop while state is not the goal or lost state\n",
    "            while s != self.n_states -1 and s!=self.n_states-2 and t <50 :\n",
    "                # Update state\n",
    "                s = next_s;\n",
    "                # Move to next state given the policy and the current state\n",
    "                next_s = self._move(s,policy[s]);\n",
    "                # Add the position in the maze corresponding to the next state\n",
    "                # to the path\n",
    "                path.append(self.states[next_s])\n",
    "                # Update time and state for next iteration\n",
    "                t +=1;\n",
    "        \n",
    "        return path\n",
    "\n",
    "def dynamic_programming(env, horizon):\n",
    "    p         = env.transition_probabilities;\n",
    "    r         = env.rewards;\n",
    "    n_states  = env.n_states;\n",
    "    n_actions = env.n_actions;\n",
    "    T         = horizon;\n",
    "\n",
    "    # The variables involved in the dynamic programming backwards recursions\n",
    "    V      = np.zeros((n_states, T+1));\n",
    "    policy = np.zeros((n_states, T+1));\n",
    "    Q      = np.zeros((n_states, n_actions));\n",
    "\n",
    "\n",
    "    # Initialization\n",
    "    Q            = np.copy(r);\n",
    "    V[:, T]      = np.max(Q,1);\n",
    "    policy[:, T] = np.argmax(Q,1);\n",
    "    V[env.n_states-2, T] = 1\n",
    "    \n",
    "    for t in range(T-1,-1,-1):\n",
    "        # Update the value function acccording to the bellman equation\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                # Update of the temporary Q values\n",
    "                Q[s,a] = r[s,a] + np.dot(p[:,s,a],V[:,t+1])\n",
    "        # Update by taking the maximum Q value w.r.t the action a\n",
    "        V[:,t] = np.max(Q,1);\n",
    "        # The optimal action is the one that maximizes the Q function\n",
    "        policy[:,t] = np.argmax(Q,1);\n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "def value_iteration(env, gamma, epsilon):\n",
    "    \"\"\" Solves the shortest path problem using value iteration\n",
    "        :input Maze env           : The maze environment in which we seek to\n",
    "                                    find the shortest path.\n",
    "        :input float gamma        : The discount factor.\n",
    "        :input float epsilon      : accuracy of the value iteration procedure.\n",
    "        :return numpy.array V     : Optimal values for every state at every\n",
    "                                    time, dimension S*T\n",
    "        :return numpy.array policy: Optimal time-varying policy at every state,\n",
    "                                    dimension S*T\n",
    "    \"\"\"\n",
    "    # The value itearation algorithm requires the knowledge of :\n",
    "    # - Transition probabilities\n",
    "    # - Rewards\n",
    "    # - State space\n",
    "    # - Action space\n",
    "    # - The finite horizon\n",
    "    p         = env.transition_probabilities;\n",
    "    r         = env.rewards;\n",
    "    n_states  = env.n_states;\n",
    "    n_actions = env.n_actions;\n",
    "\n",
    "    # Required variables and temporary ones for the VI to run\n",
    "    V   = np.zeros(n_states);\n",
    "    Q   = np.zeros((n_states, n_actions));\n",
    "    BV  = np.zeros(n_states);\n",
    "    # Iteration counter\n",
    "    n   = 0;\n",
    "    # Tolerance error\n",
    "    tol = (1 - gamma)* epsilon/gamma;\n",
    "    # Initialization of the VI\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            Q[s, a] = r[s,a] + gamma*np.dot(p[:,s,a],V);\n",
    "    BV = np.max(Q, 1);\n",
    "    # Iterate until convergence\n",
    "    while np.linalg.norm(V - BV) >= tol and n < 200:\n",
    "        # Increment by one the numbers of iteration\n",
    "        n += 1;\n",
    "        # Update the value function\n",
    "        V = np.copy(BV);\n",
    "        # Compute the new BV\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                Q[s, a] = r[s, a] + gamma*np.dot(p[:,s,a],V);\n",
    "        BV = np.max(Q, 1);\n",
    "        # Show error\n",
    "        #print(np.linalg.norm(V - BV))\n",
    "\n",
    "    # Compute policy\n",
    "    policy = np.argmax(Q,1);\n",
    "    # Return the obtained policy\n",
    "    return V, policy;\n",
    "\n",
    "def Q_learning(env, gamma, epsilon, n_episodes, n_steps):\n",
    "    rewards = env.rewards\n",
    "    start = env.map[(0,0,6,5)]\n",
    "    n_states = env.n_states\n",
    "    n_actions = env.n_actions\n",
    "    V      = np.zeros(n_states);\n",
    "    policy = np.zeros(n_states);\n",
    "    Q      = np.random.random((n_states, n_actions));\n",
    "    n      = np.ones((n_states, n_actions))\n",
    "    \n",
    "    initial_value = []\n",
    "    for episode in range(n_episodes):\n",
    "        s = start\n",
    "        initial_value.append(V[s])\n",
    "        print(f'Episode: {episode}')\n",
    "        for step in range(n_steps):\n",
    "            #Choose action epsilon-greedy\n",
    "            if np.random.uniform(0,1)< epsilon:\n",
    "                a = np.random.randint(env.n_actions)\n",
    "            else:\n",
    "                a = np.argmax(Q[s,:])\n",
    "            \n",
    "            R = rewards[s,a]\n",
    "            s_next = env._move(s,a)\n",
    "            \n",
    "            Q[s,a] = Q[s,a] + (1/n[s,a]**(2/3))*(R + gamma*np.max(Q[s_next,:]) - Q[s,a])\n",
    "            n[s,a] += 1\n",
    "            s=s_next\n",
    "            \n",
    "            \n",
    "            if s == env.map[(1,1,1,1)] or s == env.map[(1,1,1,1)]:\n",
    "                #Go to next episode\n",
    "                break\n",
    "        V[:] = np.max(Q,1)\n",
    "        policy = np.argmax(Q,1)\n",
    "        \n",
    "    return Q, policy,initial_value\n",
    "\n",
    "def animate_solution(maze, path):\n",
    "\n",
    "    # Map a color to each cell in the maze\n",
    "    col_map = {0: WHITE, 1: BLACK, 2: LIGHT_GREEN, -6: LIGHT_RED, -1: LIGHT_RED};\n",
    "\n",
    "    # Size of the maze\n",
    "    rows,cols = maze.shape;\n",
    "\n",
    "    # Create figure of the size of the maze\n",
    "    fig = plt.figure(1, figsize=(cols,rows));\n",
    "\n",
    "    # Remove the axis ticks and add title title\n",
    "    ax = plt.gca();\n",
    "    ax.set_title('Policy simulation');\n",
    "    ax.set_xticks([]);\n",
    "    ax.set_yticks([]);\n",
    "\n",
    "    # Give a color to each cell\n",
    "    colored_maze = [[col_map[maze[j,i]] for i in range(cols)] for j in range(rows)];\n",
    "\n",
    "    # Create figure of the size of the maze\n",
    "    fig = plt.figure(1, figsize=(cols,rows))\n",
    "\n",
    "    # Create a table to color\n",
    "    grid = plt.table(cellText=None,\n",
    "                     cellColours=colored_maze,\n",
    "                     cellLoc='center',\n",
    "                     loc=(0,0),\n",
    "                     edges='closed');\n",
    "\n",
    "    # Modify the hight and width of the cells in the table\n",
    "    tc = grid.properties()['children']\n",
    "    for cell in tc:\n",
    "        cell.set_height(1.0/rows);\n",
    "        cell.set_width(1.0/cols);\n",
    "\n",
    "    \n",
    "    end = tuple([i[0] for i in np.where(maze == 2)])\n",
    "    # Update the color at each frame\n",
    "    over = False\n",
    "    for i in range(len(path)):\n",
    "        if (path[i] == (1,1,1,1) or path[i] ==(-1,-1,-1,-1)) and not over:\n",
    "            over=True\n",
    "            grid.get_celld()[(path[i-1][:2])].set_facecolor(col_map[maze[path[i-1][:2]]])\n",
    "            grid.get_celld()[(path[i-1][:2])].get_text().set_text('')\n",
    "            grid.get_celld()[(path[i-1][2:])].set_facecolor(col_map[maze[path[i-1][2:]]])\n",
    "            grid.get_celld()[(path[i-1][2:])].get_text().set_text('')\n",
    "        if not over:\n",
    "            if i > 0:\n",
    "                if path[i][:2] != path[i-1][:2]:\n",
    "                    grid.get_celld()[(path[i-1][:2])].set_facecolor(col_map[maze[path[i-1][:2]]])\n",
    "                    grid.get_celld()[(path[i-1][:2])].get_text().set_text('')\n",
    "                if path[i][2:] != path[i-1][2:]:\n",
    "                    grid.get_celld()[(path[i-1][2:])].set_facecolor(col_map[maze[path[i-1][2:]]])\n",
    "                    grid.get_celld()[(path[i-1][2:])].get_text().set_text('')\n",
    "            grid.get_celld()[(path[i][:2])].set_facecolor(LIGHT_ORANGE)\n",
    "            grid.get_celld()[(path[i][:2])].get_text().set_text('Player')\n",
    "\n",
    "            grid.get_celld()[(path[i][2:])].set_facecolor(LIGHT_ORANGE)\n",
    "            grid.get_celld()[(path[i][2:])].get_text().set_text('MT')\n",
    "        \n",
    "        else:\n",
    "            if path[i] == (1,1,1,1):\n",
    "                grid.get_celld()[(end)].set_facecolor(LIGHT_GREEN)\n",
    "                grid.get_celld()[(end)].get_text().set_text('Win')\n",
    "            else:\n",
    "                grid.get_celld()[(end)].set_facecolor(LIGHT_RED)\n",
    "                grid.get_celld()[(end)].get_text().set_text('Lost')\n",
    "            \n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(0.1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "050370ff-b612-4f87-99d6-f62da345313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.array([\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 2, 0, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f6091-c03d-46cc-a239-0d3327bf5813",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze1(maze)\n",
    "start = (0,0,6,5)\n",
    "V, p = dynamic_programming(env, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fcb790-64eb-4981-8ebb-41183a0f04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = env.simulate(start,p)\n",
    "animate_solution(maze,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceaed94-8425-4b53-9348-23ebe399d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = (0,0,6,5)\n",
    "wins,losses = 0,0\n",
    "win_percentage_dp = []\n",
    "for t in range(1,31):\n",
    "    V, p = dynamic_programming(env, t)\n",
    "    for i in range(int(1e5)):\n",
    "        path = env.simulate(start,p)\n",
    "        if path[-1] == (1,1,1,1):\n",
    "            wins +=1\n",
    "        else:\n",
    "            losses +=1\n",
    "    win_percentage_dp.append(wins/(wins + losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc48ea-b12e-497d-8783-d230f012c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([i for i in range(len(win_percentage_dp))],win_percentage_dp )\n",
    "plt.title(\"Win Percentage vs Horizon\")\n",
    "plt.xlabel('Horizon')\n",
    "plt.ylabel('Percentage')\n",
    "plt.savefig('dp_win_percentage.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cddf46d-3ea1-4ddd-9e6b-9a41cd014899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount Factor \n",
    "gamma   = 29/30; \n",
    "# Accuracy treshold \n",
    "epsilon = 0.0001;\n",
    "\n",
    "env = Maze1(maze, STEP_REWARD = -1, GOAL_REWARD = 5, min_move=0)\n",
    "V, p = value_iteration(env, gamma, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab3676-11fc-450e-920c-16348b71aa3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = env.simulate(start,p,method='ValIter')\n",
    "animate_solution(maze,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98afffe-24c3-4a72-8104-6bb5222b13ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = (0,0,6,5)\n",
    "# Discount Factor \n",
    "gamma   = 29/30; \n",
    "# Accuracy treshold \n",
    "epsilon = 0.0001;\n",
    "\n",
    "wins,losses = 0,0\n",
    "win_percentage_vi = []\n",
    "env = Maze1(maze, STEP_REWARD = -1, GOAL_REWARD = 2, min_move=0)\n",
    "V, p = value_iteration(env, gamma,epsilon)\n",
    "for t in range(int(1e4)):\n",
    "    path = env.simulate(start,p,method='ValIter')\n",
    "    if path[-1] == (1,1,1,1) and len(path) - 1 <= 30:\n",
    "        wins +=1\n",
    "    else:\n",
    "        losses +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49238f6-ea96-4380-984d-127f0e46e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wins/(wins + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d231b2-faf7-46ee-baa1-e928b432a498",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = Maze1(maze, STEP_REWARD = 0, GOAL_REWARD = 1, min_move=0)\n",
    "Q, p, iv = Q_learning(env, gamma, 0.2, 50000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e091b-961f-4525-878f-78e245d825fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b46aae-96a3-4fdf-9e61-2d58fddbc9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = env.simulate(start,p,method='ValIter')\n",
    "animate_solution(maze,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2da98cd-bc61-4f40-a82f-2c962cdf8ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cfb5f6-91ad-40cf-b70d-9b39dd68188c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
