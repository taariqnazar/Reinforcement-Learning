{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f82cc0-0753-4109-b448-7ddc636c99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import sys\n",
    "sys.path.append('../lab0')\n",
    "import maze as mz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "491411db-cb16-41ba-a7a5-691e38672520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implemented methods\n",
    "methods = ['DynProg', 'ValIter'];\n",
    "\n",
    "# Some colours\n",
    "LIGHT_RED    = '#FFC4CC';\n",
    "LIGHT_GREEN  = '#95FD99';\n",
    "BLACK        = '#000000';\n",
    "WHITE        = '#FFFFFF';\n",
    "LIGHT_PURPLE = '#E8D0FF';\n",
    "LIGHT_ORANGE = '#FAE0C3';\n",
    "\n",
    "class Maze1:\n",
    "     # Actions\n",
    "    STAY       = 0\n",
    "    MOVE_LEFT  = 1\n",
    "    MOVE_RIGHT = 2\n",
    "    MOVE_UP    = 3\n",
    "    MOVE_DOWN  = 4\n",
    "\n",
    "    # Give names to actions\n",
    "    actions_names = {\n",
    "        STAY: \"stay\",\n",
    "        MOVE_LEFT: \"move left\",\n",
    "        MOVE_RIGHT: \"move right\",\n",
    "        MOVE_UP: \"move up\",\n",
    "        MOVE_DOWN: \"move down\"\n",
    "    }\n",
    "    \n",
    "    def __init__(self, maze, weights=None, STEP_REWARD = 0, GOAL_REWARD = 0, random_rewards=False, min_move=1):\n",
    "        \"\"\" Constructor of the environment Maze.\n",
    "        \"\"\"\n",
    "        self.maze                     = maze;\n",
    "        self.win_state = self.__win_state()\n",
    "        self.actions                  = self.__actions();\n",
    "        self.states, self.map         = self.__states();\n",
    "        self.n_actions                = len(self.actions);\n",
    "        self.n_states                 = len(self.states);\n",
    "        self.min_move = min_move\n",
    "        self.transition_probabilities = self.__transitions();\n",
    "        \n",
    "        self.STEP_REWARD = STEP_REWARD\n",
    "        self.GOAL_REWARD = GOAL_REWARD\n",
    "        self.rewards                  = self.__rewards(weights=weights,\n",
    "                                                random_rewards=random_rewards);\n",
    "        \n",
    "        \n",
    "    def __win_state(self):\n",
    "        return tuple([i[0] for i in np.where(self.maze == 2)])        \n",
    "        \n",
    "    def __actions(self):\n",
    "        actions = dict();\n",
    "        actions[self.STAY]       = (0, 0);\n",
    "        actions[self.MOVE_LEFT]  = (0,-1);\n",
    "        actions[self.MOVE_RIGHT] = (0, 1);\n",
    "        actions[self.MOVE_UP]    = (-1,0);\n",
    "        actions[self.MOVE_DOWN]  = (1,0);\n",
    "        return actions;\n",
    "    \n",
    "    def __states(self):\n",
    "        states = dict()\n",
    "        map = dict()\n",
    "        s = 0\n",
    "        for i in range(self.maze.shape[0]):\n",
    "            for j in range(self.maze.shape[1]):\n",
    "                if maze[i,j] !=1:\n",
    "                    for k in range(self.maze.shape[0]):\n",
    "                        for m in range(self.maze.shape[1]):\n",
    "                            if maze[k,m] !=1:\n",
    "                                if ((i,j) != (k,m) and (i,j) != self.win_state):\n",
    "                                    states[s] = (i,j,k,m)\n",
    "                                    map[(i,j,k,m)] = s\n",
    "                                    s += 1\n",
    "\n",
    "        states[s] = (1,1,1,1) # Win state \n",
    "        map[(1,1,1,1)] = s\n",
    "        s+=1\n",
    "\n",
    "        states[s] = (-1,-1,-1,-1) #Lost state\n",
    "        map[(-1,-1,-1,-1)] = s\n",
    "        return states, map\n",
    "\n",
    "    def hitting_maze_walls(self,row, col):\n",
    "        return (row == -1) or (row == self.maze.shape[0]) or \\\n",
    "                  (col == -1) or (col == self.maze.shape[1]) or \\\n",
    "                  (self.maze[row,col] == 1);\n",
    "\n",
    "    def __moves(self,state,action):\n",
    "        #If win or loose stay there\n",
    "        if state == self.map[(-1,-1,-1,-1)] or state == self.map[(1,1,1,1)]:\n",
    "            return [state]\n",
    "        s = np.array(self.states[state])\n",
    "        \n",
    "        #Check viable moves\n",
    "        p_next = tuple(s[:2] + self.actions[action])\n",
    "        if self.hitting_maze_walls(p_next[0], p_next[1]):\n",
    "                  p_next = tuple(s[:2])\n",
    "                \n",
    "        mt_temp = [tuple(s[2:] + self.actions[i]) for i in range(self.min_move,5)]\n",
    "        mt_next= []\n",
    "        for mt in mt_temp:\n",
    "            if not self.hitting_maze_walls(mt[0], mt[1]):\n",
    "                  mt_next.append(mt)\n",
    "        \n",
    "        s_next = [p_next + mt for mt in mt_next]\n",
    "\n",
    "        #Check if game is finished\n",
    "        final = []\n",
    "        for i in s_next:\n",
    "            if i[:2] == i[2:]:\n",
    "                final.append((-1,-1,-1,-1))\n",
    "            elif i[:2] == self.win_state:\n",
    "                final.append((1,1,1,1))\n",
    "            else:\n",
    "                final.append(i)\n",
    "\n",
    "        if len(final) < 1:\n",
    "            return [state]\n",
    "        else:\n",
    "            return [self.map[i] for i in final]\n",
    "        \n",
    "    def __transitions(self):\n",
    "        dimensions = (self.n_states,self.n_states,self.n_actions)\n",
    "        transition_probabilities = np.zeros(dimensions)\n",
    "\n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                next_ss = self.__moves(s,a)\n",
    "                for next_s in next_ss:\n",
    "                    transition_probabilities[next_s,s,a] += 1/len(next_ss)\n",
    "\n",
    "        return transition_probabilities\n",
    "    \n",
    "    def __rewards(self, weights=None, random_rewards=None):\n",
    "        rewards = np.zeros((self.n_states, self.n_actions))\n",
    "        \n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                next_s = self.__moves(s,a)\n",
    "                for ns in next_s:\n",
    "                    if ns == self.n_states-2:\n",
    "                        rewards[s,a] = self.GOAL_REWARD\n",
    "                    elif ns == self.n_states-1:\n",
    "                        rewards[s,a] = -self.GOAL_REWARD\n",
    "                    else:\n",
    "                        rewards[s,a] = self.STEP_REWARD\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    def __move(self,state, action):\n",
    "        next_s = self.__moves(state, action)\n",
    "        return np.random.choice(next_s)\n",
    "        \n",
    "    def simulate(self, start, policy, method='DynProg'):\n",
    "        if method not in methods:\n",
    "            error = 'ERROR: the argument method must be in {}'.format(methods);\n",
    "            raise NameError(error);\n",
    "\n",
    "        path = list();\n",
    "        if method == 'DynProg':\n",
    "            # Deduce the horizon from the policy shape\n",
    "            horizon = policy.shape[1];\n",
    "            # Initialize current state and time\n",
    "            t = 0;\n",
    "            s = self.map[start];\n",
    "            # Add the starting position in the maze to the path\n",
    "            path.append(start);\n",
    "            while t < horizon-1:\n",
    "                # Move to next state given the policy and the current state\n",
    "                next_s = self.__move(s,policy[s,t]);\n",
    "                # Add the position in the maze corresponding to the next state\n",
    "                # to the path\n",
    "                path.append(self.states[next_s])\n",
    "                # Update time and state for next iteration\n",
    "                t +=1;\n",
    "                s = next_s;\n",
    "        \n",
    "        if method == 'ValIter':\n",
    "            # Initialize current state, next state and time\n",
    "            t = 1;\n",
    "            s = self.map[start];\n",
    "            # Add the starting position in the maze to the path\n",
    "            path.append(start);\n",
    "            # Move to next state given the policy and the current state\n",
    "            next_s = self.__move(s,policy[s]);\n",
    "            # Add the position in the maze corresponding to the next state\n",
    "            # to the path\n",
    "            path.append(self.states[next_s]);\n",
    "            \n",
    "            # Loop while state is not the goal or lost state\n",
    "            while s != self.n_states -1 and s!=self.n_states-2 and t <50 :\n",
    "                # Update state\n",
    "                s = next_s;\n",
    "                # Move to next state given the policy and the current state\n",
    "                next_s = self.__move(s,policy[s]);\n",
    "                # Add the position in the maze corresponding to the next state\n",
    "                # to the path\n",
    "                path.append(self.states[next_s])\n",
    "                # Update time and state for next iteration\n",
    "                t +=1;\n",
    "        \n",
    "        return path\n",
    "\n",
    "def dynamic_programming(env, horizon):\n",
    "    p         = env.transition_probabilities;\n",
    "    r         = env.rewards;\n",
    "    n_states  = env.n_states;\n",
    "    n_actions = env.n_actions;\n",
    "    T         = horizon;\n",
    "\n",
    "    # The variables involved in the dynamic programming backwards recursions\n",
    "    V      = np.zeros((n_states, T+1));\n",
    "    policy = np.zeros((n_states, T+1));\n",
    "    Q      = np.zeros((n_states, n_actions));\n",
    "\n",
    "\n",
    "    # Initialization\n",
    "    Q            = np.copy(r);\n",
    "    V[:, T]      = np.max(Q,1);\n",
    "    policy[:, T] = np.argmax(Q,1);\n",
    "    V[env.n_states-2, T] = 1\n",
    "    \n",
    "    for t in range(T-1,-1,-1):\n",
    "        # Update the value function acccording to the bellman equation\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                # Update of the temporary Q values\n",
    "                Q[s,a] = r[s,a] + np.dot(p[:,s,a],V[:,t+1])\n",
    "        # Update by taking the maximum Q value w.r.t the action a\n",
    "        V[:,t] = np.max(Q,1);\n",
    "        # The optimal action is the one that maximizes the Q function\n",
    "        policy[:,t] = np.argmax(Q,1);\n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "def value_iteration(env, gamma, epsilon):\n",
    "    \"\"\" Solves the shortest path problem using value iteration\n",
    "        :input Maze env           : The maze environment in which we seek to\n",
    "                                    find the shortest path.\n",
    "        :input float gamma        : The discount factor.\n",
    "        :input float epsilon      : accuracy of the value iteration procedure.\n",
    "        :return numpy.array V     : Optimal values for every state at every\n",
    "                                    time, dimension S*T\n",
    "        :return numpy.array policy: Optimal time-varying policy at every state,\n",
    "                                    dimension S*T\n",
    "    \"\"\"\n",
    "    # The value itearation algorithm requires the knowledge of :\n",
    "    # - Transition probabilities\n",
    "    # - Rewards\n",
    "    # - State space\n",
    "    # - Action space\n",
    "    # - The finite horizon\n",
    "    p         = env.transition_probabilities;\n",
    "    r         = env.rewards;\n",
    "    n_states  = env.n_states;\n",
    "    n_actions = env.n_actions;\n",
    "\n",
    "    # Required variables and temporary ones for the VI to run\n",
    "    V   = np.zeros(n_states);\n",
    "    Q   = np.zeros((n_states, n_actions));\n",
    "    BV  = np.zeros(n_states);\n",
    "    # Iteration counter\n",
    "    n   = 0;\n",
    "    # Tolerance error\n",
    "    tol = (1 - gamma)* epsilon/gamma;\n",
    "    # Initialization of the VI\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            Q[s, a] = r[s,a] + gamma*np.dot(p[:,s,a],V);\n",
    "    BV = np.max(Q, 1);\n",
    "    # Iterate until convergence\n",
    "    while np.linalg.norm(V - BV) >= tol and n < 200:\n",
    "        # Increment by one the numbers of iteration\n",
    "        n += 1;\n",
    "        # Update the value function\n",
    "        V = np.copy(BV);\n",
    "        # Compute the new BV\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                Q[s, a] = r[s, a] + gamma*np.dot(p[:,s,a],V);\n",
    "        BV = np.max(Q, 1);\n",
    "        # Show error\n",
    "        #print(np.linalg.norm(V - BV))\n",
    "\n",
    "    # Compute policy\n",
    "    policy = np.argmax(Q,1);\n",
    "    # Return the obtained policy\n",
    "    return V, policy;\n",
    "\n",
    "def animate_solution(maze, path):\n",
    "\n",
    "    # Map a color to each cell in the maze\n",
    "    col_map = {0: WHITE, 1: BLACK, 2: LIGHT_GREEN, -6: LIGHT_RED, -1: LIGHT_RED};\n",
    "\n",
    "    # Size of the maze\n",
    "    rows,cols = maze.shape;\n",
    "\n",
    "    # Create figure of the size of the maze\n",
    "    fig = plt.figure(1, figsize=(cols,rows));\n",
    "\n",
    "    # Remove the axis ticks and add title title\n",
    "    ax = plt.gca();\n",
    "    ax.set_title('Policy simulation');\n",
    "    ax.set_xticks([]);\n",
    "    ax.set_yticks([]);\n",
    "\n",
    "    # Give a color to each cell\n",
    "    colored_maze = [[col_map[maze[j,i]] for i in range(cols)] for j in range(rows)];\n",
    "\n",
    "    # Create figure of the size of the maze\n",
    "    fig = plt.figure(1, figsize=(cols,rows))\n",
    "\n",
    "    # Create a table to color\n",
    "    grid = plt.table(cellText=None,\n",
    "                     cellColours=colored_maze,\n",
    "                     cellLoc='center',\n",
    "                     loc=(0,0),\n",
    "                     edges='closed');\n",
    "\n",
    "    # Modify the hight and width of the cells in the table\n",
    "    tc = grid.properties()['children']\n",
    "    for cell in tc:\n",
    "        cell.set_height(1.0/rows);\n",
    "        cell.set_width(1.0/cols);\n",
    "\n",
    "    \n",
    "    end = tuple([i[0] for i in np.where(maze == 2)])\n",
    "    # Update the color at each frame\n",
    "    over = False\n",
    "    for i in range(len(path)):\n",
    "        if (path[i] == (1,1,1,1) or path[i] ==(-1,-1,-1,-1)) and not over:\n",
    "            over=True\n",
    "            grid.get_celld()[(path[i-1][:2])].set_facecolor(col_map[maze[path[i-1][:2]]])\n",
    "            grid.get_celld()[(path[i-1][:2])].get_text().set_text('')\n",
    "            grid.get_celld()[(path[i-1][2:])].set_facecolor(col_map[maze[path[i-1][2:]]])\n",
    "            grid.get_celld()[(path[i-1][2:])].get_text().set_text('')\n",
    "        if not over:\n",
    "            if i > 0:\n",
    "                if path[i][:2] != path[i-1][:2]:\n",
    "                    grid.get_celld()[(path[i-1][:2])].set_facecolor(col_map[maze[path[i-1][:2]]])\n",
    "                    grid.get_celld()[(path[i-1][:2])].get_text().set_text('')\n",
    "                if path[i][2:] != path[i-1][2:]:\n",
    "                    grid.get_celld()[(path[i-1][2:])].set_facecolor(col_map[maze[path[i-1][2:]]])\n",
    "                    grid.get_celld()[(path[i-1][2:])].get_text().set_text('')\n",
    "            grid.get_celld()[(path[i][:2])].set_facecolor(LIGHT_ORANGE)\n",
    "            grid.get_celld()[(path[i][:2])].get_text().set_text('Player')\n",
    "\n",
    "            grid.get_celld()[(path[i][2:])].set_facecolor(LIGHT_ORANGE)\n",
    "            grid.get_celld()[(path[i][2:])].get_text().set_text('MT')\n",
    "        \n",
    "        else:\n",
    "            if path[i] == (1,1,1,1):\n",
    "                grid.get_celld()[(end)].set_facecolor(LIGHT_GREEN)\n",
    "                grid.get_celld()[(end)].get_text().set_text('Win')\n",
    "            else:\n",
    "                grid.get_celld()[(end)].set_facecolor(LIGHT_RED)\n",
    "                grid.get_celld()[(end)].get_text().set_text('Lost')\n",
    "            \n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(0.1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "050370ff-b612-4f87-99d6-f62da345313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.array([\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 2, 0, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f6091-c03d-46cc-a239-0d3327bf5813",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze1(maze)\n",
    "start = (0,0,6,5)\n",
    "V, p = dynamic_programming(env, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fcb790-64eb-4981-8ebb-41183a0f04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = env.simulate(start,p)\n",
    "animate_solution(maze,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceaed94-8425-4b53-9348-23ebe399d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = (0,0,6,5)\n",
    "wins,losses = 0,0\n",
    "win_percentage_dp = []\n",
    "for t in range(1,31):\n",
    "    V, p = dynamic_programming(env, t)\n",
    "    for i in range(int(1e5)):\n",
    "        path = env.simulate(start,p)\n",
    "        if path[-1] == (1,1,1,1):\n",
    "            wins +=1\n",
    "        else:\n",
    "            losses +=1\n",
    "    win_percentage_dp.append(wins/(wins + losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc48ea-b12e-497d-8783-d230f012c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([i for i in range(len(win_percentage_dp))],win_percentage_dp )\n",
    "plt.title(\"Win Percentage vs Horizon\")\n",
    "plt.xlabel('Horizon')\n",
    "plt.ylabel('Percentage')\n",
    "plt.savefig('dp_win_percentage.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9cddf46d-3ea1-4ddd-9e6b-9a41cd014899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount Factor \n",
    "gamma   = 29/30; \n",
    "# Accuracy treshold \n",
    "epsilon = 0.0001;\n",
    "\n",
    "env = Maze1(maze, STEP_REWARD = -1, GOAL_REWARD = 5, min_move=0)\n",
    "V, p = value_iteration(env, gamma, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "19ab3676-11fc-450e-920c-16348b71aa3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = env.simulate(start,p,method='ValIter')\n",
    "animate_solution(maze,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b98afffe-24c3-4a72-8104-6bb5222b13ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = (0,0,6,5)\n",
    "# Discount Factor \n",
    "gamma   = 29/30; \n",
    "# Accuracy treshold \n",
    "epsilon = 0.0001;\n",
    "\n",
    "wins,losses = 0,0\n",
    "win_percentage_vi = []\n",
    "env = Maze1(maze, STEP_REWARD = -1, GOAL_REWARD = 2, min_move=0)\n",
    "V, p = value_iteration(env, gamma,epsilon)\n",
    "for t in range(int(1e4)):\n",
    "    path = env.simulate(start,p,method='ValIter')\n",
    "    if path[-1] == (1,1,1,1) and len(path) - 1 <= 30:\n",
    "        wins +=1\n",
    "    else:\n",
    "        losses +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c49238f6-ea96-4380-984d-127f0e46e14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.578"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wins/(wins + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d231b2-faf7-46ee-baa1-e928b432a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze1(maze, STEP_REWARD = -1, GOAL_REWARD = 2, min_move=0)\n",
    "Q,p = Q_learning(env, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "d2da98cd-bc61-4f40-a82f-2c962cdf8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env, alpha, gamma, epsilon, n_episodes, n_steps):\n",
    "    reward = env.rewards\n",
    "    start = env.map[(0,0,6,5)]\n",
    "    \n",
    "    V      = np.zeros(n_states);\n",
    "    policy = np.zeros(n_states);\n",
    "    Q      = np.zeros((n_states, n_actions));\n",
    "    n =np.ones((n_states, n_actions))\n",
    "    \n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        s = start\n",
    "        print(f'Episode: {episode}')\n",
    "        for step in range(n_steps):\n",
    "            #Choose action epsilon-greedy\n",
    "            if np.random.uniform(0,1)< epsilon:\n",
    "                a = np.random.randint(env.n_actions)\n",
    "            else:\n",
    "                a = np.argmax(Q[s,:])\n",
    "            \n",
    "            R = rewards[s,a]\n",
    "            s_next = env.__move(s,a)\n",
    "            \n",
    "            Q[s,a] = Q[s,a] + alpha*(R + gamma*np.max(Q[s_next,:]) - Q[s,a])\n",
    "            n[s,a] += 1\n",
    "            s=s_next\n",
    "            if s == env.map[(1,1,1,1)] or s == env.map[(1,1,1,1)]:\n",
    "                #Go to next episode\n",
    "                break\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b3d54fc0-87b4-4572-ab9f-7c06db336130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(0,1)< epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d91bb96e-6649-4d98-945f-de5484d4947d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8370599b-081c-4ccf-8aa5-e7c24965e7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
